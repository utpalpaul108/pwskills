{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assignment on Bag of Words and TF, IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Used dataset : https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(os.path.join('dataset', 'imdbMovieReview','imdbDataset.csv'))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-MnjNsGhDfCA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "review       0\n",
              "sentiment    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Problem 1\n",
        "# Apply all the preprocessing techniques that you think are necessary\n",
        "\n",
        "# Check null values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check duplicates values\n",
        "df.duplicated().sum()\n",
        "\n",
        "# Remove duplicates values\n",
        "df = df.drop_duplicates()\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production br br the filmin...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically theres a family where a little boy j...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter matteis love in the time of money is a ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  one of the other reviewers has mentioned that ...  positive\n",
              "1  a wonderful little production br br the filmin...  positive\n",
              "2  i thought this was a wonderful way to spend ti...  positive\n",
              "3  basically theres a family where a little boy j...  negative\n",
              "4  petter matteis love in the time of money is a ...  positive"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Lowercasing\n",
        "df['review'] = df['review'].str.lower()\n",
        "\n",
        "# Removing Whitespace\n",
        "df['review'] = df['review'].str.strip()\n",
        "\n",
        "# Remove HTML tags\n",
        "df['review'] = df['review'].str.replace('<.*?>','')\n",
        "\n",
        "# Remove URLs\n",
        "df['review'] = df['review'].str.replace('https?://\\S+|www\\.\\S+','')\n",
        "\n",
        "# Remove Punctuation\n",
        "import string\n",
        "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "df['review'] = df['review'].str.translate(translator)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuVSlxaZDqsj"
      },
      "outputs": [],
      "source": [
        "# Problem 2\n",
        "# Find out the number of words in the entire corpus and also the total number of unique words(vocabulary) using just python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "total_words = []\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for review in df['review']:\n",
        "    words = [lemmatizer.lemmatize(word, pos='v') for word in word_tokenize(review) if word not in stopwords.words('english')]\n",
        "    total_words.extend(words)\n",
        "\n",
        "vocabulary = list(set(total_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words:  6094923\n",
            "Vocabulary:  168055\n"
          ]
        }
      ],
      "source": [
        "print('Total words: ', len(total_words))\n",
        "print('Vocabulary: ', len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "B9BcW8aLD4nr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(171, 168055)\n"
          ]
        }
      ],
      "source": [
        "# Problem 3\n",
        "# Apply One Hot Encoding\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create the encoder\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "\n",
        "data = [lemmatizer.lemmatize(word, pos='v') for word in word_tokenize(df['review'][0]) if word not in stopwords.words('english')]\n",
        "data = np.array(data).reshape(-1,1)\n",
        "\n",
        "# Fit the encoder to the vocabulary\n",
        "encoder.fit(np.array(vocabulary).reshape(-1,1))\n",
        "\n",
        "\n",
        "# Transform the text data into one-hot encoded vectors\n",
        "one_hot_encoded_data = encoder.transform(data).toarray()\n",
        "\n",
        "print(one_hot_encoded_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "171\n",
            "168055\n"
          ]
        }
      ],
      "source": [
        "print(len(data))\n",
        "print(len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tSQ9sf6NFSTb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['about' 'accustomed' 'after' 'agenda' 'agreements' 'all' 'an' 'and'\n",
            " 'appeal' 'are' 'around' 'as' 'audiences' 'away' 'awaybr' 'be' 'become'\n",
            " 'being' 'bitches' 'br' 'brutality' 'but' 'called' 'can' 'cells' 'charm'\n",
            " 'christians' 'city' 'class' 'classic' 'comfortable' 'couldnt' 'crooked'\n",
            " 'dare' 'darker' 'dealings' 'death' 'developed' 'dodgy' 'doesnt' 'drugs'\n",
            " 'due' 'em' 'emerald' 'episode' 'ever' 'exactly' 'experience'\n",
            " 'experimental' 'face' 'fact' 'faint' 'far' 'first' 'focuses' 'for'\n",
            " 'forget' 'from' 'fronts' 'gangstas' 'get' 'given' 'glass' 'go' 'goes'\n",
            " 'got' 'graphic' 'guards' 'happened' 'hardcore' 'has' 'have' 'hearted'\n",
            " 'high' 'home' 'hooked' 'if' 'in' 'injustice' 'inmates' 'into' 'inwards'\n",
            " 'irish' 'is' 'it' 'italians' 'its' 'just' 'kill' 'lack' 'latinos'\n",
            " 'levels' 'main' 'mainly' 'mainstream' 'mannered' 'manyaryans' 'maximum'\n",
            " 'may' 'me' 'mebr' 'mentioned' 'mess' 'middle' 'more' 'moreso' 'muslims'\n",
            " 'nasty' 'never' 'nickel' 'nickname' 'no' 'not' 'of' 'on' 'one' 'or'\n",
            " 'order' 'oswald' 'other' 'out' 'oz' 'painted' 'penitentary' 'pictures'\n",
            " 'pretty' 'prison' 'privacy' 'pulls' 'punches' 'ready' 'regards'\n",
            " 'reviewers' 'right' 'romanceoz' 'saw' 'say' 'scenes' 'scuffles' 'section'\n",
            " 'security' 'set' 'sex' 'shady' 'show' 'shows' 'side' 'skills' 'so' 'sold'\n",
            " 'stares' 'state' 'street' 'struck' 'surreal' 'taste' 'that' 'the' 'their'\n",
            " 'they' 'thing' 'this' 'timid' 'to' 'touch' 'trust' 'turned'\n",
            " 'uncomfortable' 'unflinching' 'use' 'viewingthats' 'violence' 'was'\n",
            " 'watched' 'watching' 'well' 'what' 'where' 'which' 'wholl' 'with' 'word'\n",
            " 'wordbr' 'would' 'wouldnt' 'you' 'youll' 'your']\n"
          ]
        }
      ],
      "source": [
        "# Problem 4\n",
        "# Apply bag words and find the vocabulary also find the times each word has occured\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "sentences = sent_tokenize(df['review'][0])\n",
        "\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Vocabulary\n",
        "print(vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1  1  1  1  1  1  1  6  1  2  1  4  1  1  1  2  1  1  1  3  1  2  1  1\n",
            "   1  1  1  2  1  1  1  1  1  1  1  1  1  1  1  1  1  2  1  1  2  1  1  1\n",
            "   1  1  1  1  1  2  1  5  3  1  1  1  2  1  1  1  1  1  1  1  1  1  1  1\n",
            "   1  2  1  1  1  3  1  2  1  1  1  9  6  1  2  2  1  1  1  1  1  1  1  1\n",
            "   1  1  1  3  1  1  1  1  1  1  1  1  1  1  1  1  3  7  3  1  3  1  1  2\n",
            "   1  5  1  1  1  1  3  1  1  1  1  1  1  2  1  1  2  1  1  1  1  1  1  1\n",
            "   3  1  1  1  2  1  1  1  1  2  1  1  4 16  1  1  1  3  1  6  1  1  1  1\n",
            "   1  1  1  4  3  1  2  1  2  2  1  2  5  1  1  1  1  2  1  1]]\n",
            "(1, 188)\n",
            "188\n"
          ]
        }
      ],
      "source": [
        "# Bag of words\n",
        "print(X.toarray())\n",
        "print(X.toarray().shape)\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "vR8LnGwZFbhD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['about oz' 'accustomed to' 'after watching' 'agenda em' 'agreements are'\n",
            " 'all the' 'an experimental' 'and face' 'and get' 'and got' 'and moreso'\n",
            " 'and shady' 'and unflinching' 'appeal of' 'are never' 'are right'\n",
            " 'around the' 'as so' 'as that' 'as this' 'as watched' 'audiences forget'\n",
            " 'away with' 'awaybr br' 'be hooked' 'be sold' 'become comfortable'\n",
            " 'being turned' 'bitches due' 'br it' 'br the' 'br would' 'brutality and'\n",
            " 'but as' 'but injustice' 'called oz' 'can get' 'cells have'\n",
            " 'charm forget' 'christians italians' 'city an' 'city is' 'class inmates'\n",
            " 'classic use' 'comfortable with' 'couldnt say' 'crooked guards'\n",
            " 'dare forget' 'darker side' 'dealings and' 'death stares'\n",
            " 'developed taste' 'dodgy dealings' 'doesnt mess' 'drugs sex' 'due to'\n",
            " 'em city' 'emerald city' 'episode ever' 'episode youll' 'ever saw'\n",
            " 'exactly what' 'experience watching' 'experimental section'\n",
            " 'face inwards' 'fact that' 'faint hearted' 'far awaybr' 'first episode'\n",
            " 'first thing' 'focuses mainly' 'for it' 'for mainstream' 'for nickel'\n",
            " 'for oz' 'for the' 'forget charm' 'forget pretty' 'forget romanceoz'\n",
            " 'from the' 'fronts and' 'gangstas latinos' 'get away' 'get in' 'given to'\n",
            " 'glass fronts' 'go trust' 'goes where' 'got accustomed'\n",
            " 'graphic violence' 'guards wholl' 'happened with' 'hardcore in'\n",
            " 'has mentioned' 'have glass' 'hearted or' 'high levels' 'high on'\n",
            " 'home to' 'hooked they' 'if you' 'in right' 'in the' 'in touch'\n",
            " 'injustice crooked' 'inmates being' 'inmates wholl' 'into prison'\n",
            " 'inwards so' 'irish and' 'is called' 'is due' 'is exactly' 'is hardcore'\n",
            " 'is home' 'is not' 'is the' 'is uncomfortable' 'it but' 'it focuses'\n",
            " 'it goes' 'it is' 'it was' 'it well' 'italians irish' 'its brutality'\n",
            " 'its is' 'just oz' 'just violence' 'kill on' 'lack of'\n",
            " 'latinos christians' 'levels of' 'main appeal' 'mainly on'\n",
            " 'mainstream audiences' 'mannered middle' 'manyaryans muslims'\n",
            " 'maximum security' 'may become' 'me about' 'me as' 'me this' 'mebr br'\n",
            " 'mentioned that' 'mess around' 'middle class' 'more developed'\n",
            " 'moreso scuffles' 'muslims gangstas' 'nasty it' 'never far'\n",
            " 'nickel inmates' 'nickname given' 'no punches' 'not high' 'not just'\n",
            " 'not show' 'of graphic' 'of street' 'of the' 'of violence' 'on emerald'\n",
            " 'on order' 'on the' 'one of' 'or prison' 'or timid' 'or violence'\n",
            " 'order and' 'oswald maximum' 'other reviewers' 'other shows' 'out for'\n",
            " 'oz and' 'oz as' 'oz episode' 'oz was' 'oz you' 'painted for'\n",
            " 'penitentary it' 'pictures painted' 'pretty pictures' 'prison bitches'\n",
            " 'prison experience' 'prison where' 'privacy is' 'pulls no' 'punches with'\n",
            " 'ready for' 'regards to' 'reviewers has' 'right as' 'right from'\n",
            " 'romanceoz doesnt' 'saw struck' 'say the' 'say was' 'scenes of'\n",
            " 'scuffles death' 'section of' 'security state' 'set in' 'sex or'\n",
            " 'shady agreements' 'show for' 'show is' 'show pulls' 'shows wouldnt'\n",
            " 'skills or' 'so nasty' 'so privacy' 'sold out' 'stares dodgy'\n",
            " 'state penitentary' 'street skills' 'struck me' 'surreal couldnt'\n",
            " 'taste for' 'that after' 'that is' 'that it' 'that struck' 'the agenda'\n",
            " 'the cells' 'the classic' 'the fact' 'the faint' 'the first' 'the high'\n",
            " 'the main' 'the nickname' 'the oswald' 'the other' 'the prison'\n",
            " 'the show' 'the word' 'the wordbr' 'their lack' 'they are' 'thing that'\n",
            " 'this is' 'this show' 'timid this' 'to drugs' 'to manyaryans' 'to the'\n",
            " 'to their' 'touch with' 'trust me' 'turned into'\n",
            " 'uncomfortable viewingthats' 'unflinching scenes' 'use of'\n",
            " 'viewingthats if' 'violence but' 'violence its' 'violence not'\n",
            " 'violence which' 'was its' 'was ready' 'was surreal' 'watched more'\n",
            " 'watching just' 'watching oz' 'well mannered' 'what happened' 'what is'\n",
            " 'where all' 'where other' 'which set' 'wholl be' 'wholl kill' 'with it'\n",
            " 'with mebr' 'with regards' 'with what' 'with your' 'word go' 'wordbr br'\n",
            " 'would say' 'wouldnt dare' 'you can' 'you may' 'youll be' 'your darker']\n"
          ]
        }
      ],
      "source": [
        "# Problem 5\n",
        "# Apply bag of bi-gram and bag of tri-gram and write down your observation about the dimensionality of the vocabulary\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# bi-gram\n",
        "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
        "sentences = sent_tokenize(df['review'][0])\n",
        "\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Vocabulary\n",
        "print(vectorizer.get_feature_names_out())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 3 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
            "(1, 286)\n",
            "286\n"
          ]
        }
      ],
      "source": [
        "print(X.toarray())\n",
        "print(X.toarray().shape)\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['about oz was' 'accustomed to the' 'after watching just' 'agenda em city'\n",
            " 'agreements are never' 'all the cells' 'an experimental section'\n",
            " 'and face inwards' 'and get away' 'and got accustomed'\n",
            " 'and moreso scuffles' 'and shady agreements' 'and unflinching scenes'\n",
            " 'appeal of the' 'are never far' 'are right as' 'around the first'\n",
            " 'as so nasty' 'as that is' 'as this is' 'as watched more'\n",
            " 'audiences forget charm' 'away with it' 'awaybr br would'\n",
            " 'be hooked they' 'be sold out' 'become comfortable with'\n",
            " 'being turned into' 'bitches due to' 'br it is' 'br the first'\n",
            " 'br would say' 'brutality and unflinching' 'but as watched'\n",
            " 'but injustice crooked' 'called oz as' 'can get in' 'cells have glass'\n",
            " 'charm forget romanceoz' 'christians italians irish'\n",
            " 'city an experimental' 'city is home' 'class inmates being'\n",
            " 'classic use of' 'comfortable with what' 'couldnt say was'\n",
            " 'crooked guards wholl' 'dare forget pretty' 'dealings and shady'\n",
            " 'death stares dodgy' 'developed taste for' 'dodgy dealings and'\n",
            " 'doesnt mess around' 'drugs sex or' 'due to the' 'due to their'\n",
            " 'em city is' 'emerald city an' 'episode ever saw' 'episode youll be'\n",
            " 'ever saw struck' 'exactly what happened' 'experience watching oz'\n",
            " 'experimental section of' 'face inwards so' 'fact that it'\n",
            " 'faint hearted or' 'far awaybr br' 'first episode ever'\n",
            " 'first thing that' 'focuses mainly on' 'for it but'\n",
            " 'for mainstream audiences' 'for nickel inmates' 'for oz and'\n",
            " 'for the faint' 'forget charm forget' 'forget pretty pictures'\n",
            " 'forget romanceoz doesnt' 'from the word' 'fronts and face'\n",
            " 'gangstas latinos christians' 'get away with' 'get in touch'\n",
            " 'given to the' 'glass fronts and' 'go trust me' 'goes where other'\n",
            " 'got accustomed to' 'graphic violence not' 'guards wholl be'\n",
            " 'happened with mebr' 'hardcore in the' 'has mentioned that'\n",
            " 'have glass fronts' 'hearted or timid' 'high levels of' 'high on the'\n",
            " 'home to manyaryans' 'hooked they are' 'if you can' 'in right from'\n",
            " 'in the classic' 'in touch with' 'injustice crooked guards'\n",
            " 'inmates being turned' 'inmates wholl kill' 'into prison bitches'\n",
            " 'inwards so privacy' 'irish and moreso' 'is called oz' 'is due to'\n",
            " 'is exactly what' 'is hardcore in' 'is home to' 'is not high'\n",
            " 'is not show' 'is the nickname' 'is uncomfortable viewingthats'\n",
            " 'it but as' 'it focuses mainly' 'it goes where' 'it is called'\n",
            " 'it was surreal' 'it well mannered' 'italians irish and'\n",
            " 'its brutality and' 'its is hardcore' 'just oz episode'\n",
            " 'just violence but' 'kill on order' 'lack of street'\n",
            " 'latinos christians italians' 'levels of graphic' 'main appeal of'\n",
            " 'mainly on emerald' 'mainstream audiences forget' 'mannered middle class'\n",
            " 'manyaryans muslims gangstas' 'maximum security state'\n",
            " 'may become comfortable' 'me about oz' 'me as so' 'me this is'\n",
            " 'mebr br the' 'mentioned that after' 'mess around the'\n",
            " 'middle class inmates' 'more developed taste' 'moreso scuffles death'\n",
            " 'muslims gangstas latinos' 'nasty it was' 'never far awaybr'\n",
            " 'nickel inmates wholl' 'nickname given to' 'no punches with'\n",
            " 'not high on' 'not just violence' 'not show for' 'of graphic violence'\n",
            " 'of street skills' 'of the other' 'of the prison' 'of the show'\n",
            " 'of the wordbr' 'of violence which' 'on emerald city' 'on order and'\n",
            " 'on the agenda' 'one of the' 'or prison experience' 'or timid this'\n",
            " 'or violence its' 'order and get' 'oswald maximum security'\n",
            " 'other reviewers has' 'other shows wouldnt' 'out for nickel' 'oz and got'\n",
            " 'oz as that' 'oz episode youll' 'oz was its' 'oz you may'\n",
            " 'painted for mainstream' 'penitentary it focuses' 'pictures painted for'\n",
            " 'pretty pictures painted' 'prison bitches due'\n",
            " 'prison experience watching' 'prison where all' 'privacy is not'\n",
            " 'pulls no punches' 'punches with regards' 'ready for it'\n",
            " 'regards to drugs' 'reviewers has mentioned' 'right as this'\n",
            " 'right from the' 'romanceoz doesnt mess' 'saw struck me' 'say the main'\n",
            " 'say was ready' 'scenes of violence' 'scuffles death stares'\n",
            " 'section of the' 'security state penitentary' 'set in right'\n",
            " 'sex or violence' 'shady agreements are' 'show for the' 'show is due'\n",
            " 'show pulls no' 'shows wouldnt dare' 'skills or prison' 'so nasty it'\n",
            " 'so privacy is' 'sold out for' 'stares dodgy dealings'\n",
            " 'state penitentary it' 'street skills or' 'struck me about'\n",
            " 'struck me as' 'surreal couldnt say' 'taste for oz' 'that after watching'\n",
            " 'that is the' 'that it goes' 'that struck me' 'the agenda em'\n",
            " 'the cells have' 'the classic use' 'the fact that' 'the faint hearted'\n",
            " 'the first episode' 'the first thing' 'the high levels' 'the main appeal'\n",
            " 'the nickname given' 'the oswald maximum' 'the other reviewers'\n",
            " 'the prison where' 'the show is' 'the word go' 'the wordbr br'\n",
            " 'their lack of' 'they are right' 'thing that struck' 'this is exactly'\n",
            " 'this is not' 'this show pulls' 'timid this show' 'to drugs sex'\n",
            " 'to manyaryans muslims' 'to the fact' 'to the high' 'to the oswald'\n",
            " 'to their lack' 'touch with your' 'trust me this' 'turned into prison'\n",
            " 'uncomfortable viewingthats if' 'unflinching scenes of' 'use of the'\n",
            " 'viewingthats if you' 'violence but injustice' 'violence its is'\n",
            " 'violence not just' 'violence which set' 'was its brutality'\n",
            " 'was ready for' 'was surreal couldnt' 'watched more developed'\n",
            " 'watching just oz' 'watching oz you' 'well mannered middle'\n",
            " 'what happened with' 'what is uncomfortable' 'where all the'\n",
            " 'where other shows' 'which set in' 'wholl be sold' 'wholl kill on'\n",
            " 'with it well' 'with mebr br' 'with regards to' 'with what is'\n",
            " 'with your darker' 'word go trust' 'wordbr br it' 'would say the'\n",
            " 'wouldnt dare forget' 'you can get' 'you may become' 'youll be hooked'\n",
            " 'your darker side']\n"
          ]
        }
      ],
      "source": [
        "# tri-gram\n",
        "vectorizer = CountVectorizer(ngram_range=(3,3))\n",
        "sentences = sent_tokenize(df['review'][0])\n",
        "\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Vocabulary\n",
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1]]\n",
            "(1, 295)\n",
            "295\n"
          ]
        }
      ],
      "source": [
        "print(X.toarray())\n",
        "print(X.toarray().shape)\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "coyvsD3OFrB4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['about oz' 'accustomed to' 'after watching' 'agenda em' 'agreements are'\n",
            " 'all the' 'an experimental' 'and face' 'and get' 'and got' 'and moreso'\n",
            " 'and shady' 'and unflinching' 'appeal of' 'are never' 'are right'\n",
            " 'around the' 'as so' 'as that' 'as this' 'as watched' 'audiences forget'\n",
            " 'away with' 'awaybr br' 'be hooked' 'be sold' 'become comfortable'\n",
            " 'being turned' 'bitches due' 'br it' 'br the' 'br would' 'brutality and'\n",
            " 'but as' 'but injustice' 'called oz' 'can get' 'cells have'\n",
            " 'charm forget' 'christians italians' 'city an' 'city is' 'class inmates'\n",
            " 'classic use' 'comfortable with' 'couldnt say' 'crooked guards'\n",
            " 'dare forget' 'darker side' 'dealings and' 'death stares'\n",
            " 'developed taste' 'dodgy dealings' 'doesnt mess' 'drugs sex' 'due to'\n",
            " 'em city' 'emerald city' 'episode ever' 'episode youll' 'ever saw'\n",
            " 'exactly what' 'experience watching' 'experimental section'\n",
            " 'face inwards' 'fact that' 'faint hearted' 'far awaybr' 'first episode'\n",
            " 'first thing' 'focuses mainly' 'for it' 'for mainstream' 'for nickel'\n",
            " 'for oz' 'for the' 'forget charm' 'forget pretty' 'forget romanceoz'\n",
            " 'from the' 'fronts and' 'gangstas latinos' 'get away' 'get in' 'given to'\n",
            " 'glass fronts' 'go trust' 'goes where' 'got accustomed'\n",
            " 'graphic violence' 'guards wholl' 'happened with' 'hardcore in'\n",
            " 'has mentioned' 'have glass' 'hearted or' 'high levels' 'high on'\n",
            " 'home to' 'hooked they' 'if you' 'in right' 'in the' 'in touch'\n",
            " 'injustice crooked' 'inmates being' 'inmates wholl' 'into prison'\n",
            " 'inwards so' 'irish and' 'is called' 'is due' 'is exactly' 'is hardcore'\n",
            " 'is home' 'is not' 'is the' 'is uncomfortable' 'it but' 'it focuses'\n",
            " 'it goes' 'it is' 'it was' 'it well' 'italians irish' 'its brutality'\n",
            " 'its is' 'just oz' 'just violence' 'kill on' 'lack of'\n",
            " 'latinos christians' 'levels of' 'main appeal' 'mainly on'\n",
            " 'mainstream audiences' 'mannered middle' 'manyaryans muslims'\n",
            " 'maximum security' 'may become' 'me about' 'me as' 'me this' 'mebr br'\n",
            " 'mentioned that' 'mess around' 'middle class' 'more developed'\n",
            " 'moreso scuffles' 'muslims gangstas' 'nasty it' 'never far'\n",
            " 'nickel inmates' 'nickname given' 'no punches' 'not high' 'not just'\n",
            " 'not show' 'of graphic' 'of street' 'of the' 'of violence' 'on emerald'\n",
            " 'on order' 'on the' 'one of' 'or prison' 'or timid' 'or violence'\n",
            " 'order and' 'oswald maximum' 'other reviewers' 'other shows' 'out for'\n",
            " 'oz and' 'oz as' 'oz episode' 'oz was' 'oz you' 'painted for'\n",
            " 'penitentary it' 'pictures painted' 'pretty pictures' 'prison bitches'\n",
            " 'prison experience' 'prison where' 'privacy is' 'pulls no' 'punches with'\n",
            " 'ready for' 'regards to' 'reviewers has' 'right as' 'right from'\n",
            " 'romanceoz doesnt' 'saw struck' 'say the' 'say was' 'scenes of'\n",
            " 'scuffles death' 'section of' 'security state' 'set in' 'sex or'\n",
            " 'shady agreements' 'show for' 'show is' 'show pulls' 'shows wouldnt'\n",
            " 'skills or' 'so nasty' 'so privacy' 'sold out' 'stares dodgy'\n",
            " 'state penitentary' 'street skills' 'struck me' 'surreal couldnt'\n",
            " 'taste for' 'that after' 'that is' 'that it' 'that struck' 'the agenda'\n",
            " 'the cells' 'the classic' 'the fact' 'the faint' 'the first' 'the high'\n",
            " 'the main' 'the nickname' 'the oswald' 'the other' 'the prison'\n",
            " 'the show' 'the word' 'the wordbr' 'their lack' 'they are' 'thing that'\n",
            " 'this is' 'this show' 'timid this' 'to drugs' 'to manyaryans' 'to the'\n",
            " 'to their' 'touch with' 'trust me' 'turned into'\n",
            " 'uncomfortable viewingthats' 'unflinching scenes' 'use of'\n",
            " 'viewingthats if' 'violence but' 'violence its' 'violence not'\n",
            " 'violence which' 'was its' 'was ready' 'was surreal' 'watched more'\n",
            " 'watching just' 'watching oz' 'well mannered' 'what happened' 'what is'\n",
            " 'where all' 'where other' 'which set' 'wholl be' 'wholl kill' 'with it'\n",
            " 'with mebr' 'with regards' 'with what' 'with your' 'word go' 'wordbr br'\n",
            " 'would say' 'wouldnt dare' 'you can' 'you may' 'youll be' 'your darker']\n"
          ]
        }
      ],
      "source": [
        "# Problem 6\n",
        "# Apply tf-idf and find out the idf scores of words, also find out the vocabulary.\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# bi-gram\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
        "sentences = sent_tokenize(df['review'][0])\n",
        "\n",
        "X_tf_idf = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Vocabulary\n",
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "n7abDAiFF2Ij"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.11111111 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.11111111 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.22222222 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.11111111 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.11111111 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.11111111 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.16666667 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n",
            "  0.05555556 0.05555556 0.05555556 0.05555556]]\n",
            "(1, 286)\n",
            "286\n"
          ]
        }
      ],
      "source": [
        "print(X_tf_idf.toarray())\n",
        "print(X_tf_idf.toarray().shape)\n",
        "print(len(vectorizer.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "print(vectorizer.idf_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
